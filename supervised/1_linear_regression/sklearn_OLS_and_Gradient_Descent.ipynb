{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a11b7043-3c18-49cf-92de-43e05e4669c5",
   "metadata": {},
   "source": [
    "# Linear Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bdd8c3-8898-411b-9011-81e6924f49a4",
   "metadata": {},
   "source": [
    "## Implementation in sci-kit learn\n",
    "\n",
    "Version 1.0.3 of the scikit-learn library has two different linear regression models: one that uses OLS and another that uses a variation of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af47796-a233-4fe6-a558-badd0431a2ac",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares\n",
    "\n",
    "The LinearRegression model uses OLS. For most applications this is a good approach. Even if a data set has hundreds of predictor variables or thousands of observations, your computer will have no problem computing the parameters using OLS. One advantage of OLS is that it is **guaranteed to find the exact optimal parameters for linear regression**. Another advantage is that you don’t have to worry about what the learning rate is or whether the gradient descent algorithm will converge.\n",
    "\n",
    "Here’s some code that uses LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e025963-0460-4ec3-9b61-7bb250b7a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b1019b-363a-4dba-b1c3-6b889ccc7ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data set\n",
    "X, y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeb7189c-a7a4-46e2-92e0-28ef57a43fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -10.0098663  -239.81564367  519.84592005  324.3846455  -792.17563855\n",
      "  476.73902101  101.04326794  177.06323767  751.27369956   67.62669218]\n",
      "0.5177484222203498\n"
     ]
    }
   ],
   "source": [
    "# Create the OLS linear regression model\n",
    "ols = LinearRegression()\n",
    "\n",
    "\n",
    "# Fit the model to the data\n",
    "ols.fit(X, y)\n",
    "\n",
    "\n",
    "# Print the coefficients of the model\n",
    "print(ols.coef_)\n",
    "\n",
    "\n",
    "# Print R^2\n",
    "print(ols.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf7e13-b058-445f-9af2-df5091c1c518",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Scikit-learn’s SGDRegressor model uses a variant of gradient descent called stochastic gradient descent (or SGD for short). SGD is very similar to gradient descent, but instead of using the actual gradient it uses an approximation of the gradient that is more efficient to compute. This model is also sophisticated enough to adjust the learning rate as the SGD algorithm iterates, so in many cases you won’t have to worry about setting the learning rate.\n",
    "\n",
    "SGDRegressor also uses a technique called regularization that encourages the model to find smaller parameters. Regularization is beyond the scope of this article, but it’s important to note that the use of regularization can sometimes result in finding different coefficients than OLS would have.\n",
    "\n",
    "**If your data set is simply too large for your computer to handle OLS, you can use SGDRegressor. It will not find the exact optimal parameters, but it will get close enough for all practical purposes and it will do so without using too much computing power.** \n",
    "\n",
    "Here’s an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cec36dc5-8571-45c1-a7de-9bc0b602f53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  13.20925913 -174.67575821  460.30248544  289.0409847   -31.96825226\n",
      "  -92.86256473 -202.42859655  130.2586078   383.84128416  124.59945089]\n",
      "0.5072418527869915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Import the data set\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "# Create the SGD linear regression model\n",
    "# max_iter is the maximum number of iterations of SGD to try before halting\n",
    "sgd = SGDRegressor(max_iter = 10000)\n",
    "\n",
    "# Fit the model to the data\n",
    "sgd.fit(X, y)\n",
    "\n",
    "# Print the coefficients of the model\n",
    "print(sgd.coef_)\n",
    "\n",
    "# Print R^2\n",
    "print(sgd.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb0b13-c2ce-4eb4-aaea-0c6fb06f086a",
   "metadata": {},
   "source": [
    "### Gradient Descent in Other Machine Learning Algorithms\n",
    "\n",
    "Gradient descent can be used for much more than just linear regression. In fact, it can be used to train any machine learning algorithm as long as the ML algorithm has a loss function that is a differentiable function of the ML algorithm’s parameters. In more intuitive terms, gradient descent can be used whenever the loss function looks like smooth terrain with hills and valleys (even if those hills and valleys live in a space with more than 3 dimensions).\n",
    "\n",
    "Gradient descent (or variations of it) can be used to find parameters in **logistic regression models, support vector machines, neural networks, and other ML models**. Gradient descent’s flexibility makes it an essential part of machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f0dc0d-614f-42b3-a23e-b56d27f5875b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
